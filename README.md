# Sparkify Analytics Data Warehouse

## Objective

Create a pipeline that create and fill the Sparkify Data Warehouse 
automatically.

## Datasets

The data used in the ETL pipeline is from the original files 
(in JSON), generated by the music streaming app. There are two
datasets, described bellow:

- Songs Dataset: Contains metadata about a song and the artist
of that song.

```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

- Log Dataset: Activity from the music app. 


## Running 

To run the project you are going to need Airflow or a docker 
environment, docker configuration describe bellow.

You can run the DAG sparkify_analytics_dag on Airflow web interaface.

## Docker Enviroment 

There are two containers, one running airflow, the other postgres.
Run Airflow container with docker-compose.

```commandline
$ docker-compose up -d
```

Stop the containers:

```commandline
$ docker-compose stop
```

To use Airflow web interface, enter on http://localhost:8080/admin

## References

- [http://www.marknagelberg.com/getting-started-with-airflow-using-docker/]
- [https://github.com/puckel/docker-airflow]